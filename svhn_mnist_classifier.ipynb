{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "\n",
    "from datasets import limits\n",
    "\n",
    "from PIL import Image\n",
    "from utils import NormalizeRangeTanh, UnNormalizeRangeTanh\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import digits_model\n",
    "import torchvision\n",
    "\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroPadBottom(object):\n",
    "    ''' Zero pads batch of image tensor Variables on bottom to given size. Input (B, C, H, W) - padded on H axis. '''\n",
    "    def __init__(self, size, use_gpu=True):\n",
    "        self.size = size\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        B, C, H, W = sample.size()\n",
    "        diff = self.size - H\n",
    "        padding = Variable(torch.zeros(B, C, diff, W), requires_grad=False)\n",
    "        if self.use_gpu:\n",
    "            padding = padding.cuda()\n",
    "        zero_padded = torch.cat((sample, padding), dim=2)\n",
    "        return zero_padded\n",
    "unnormRange = UnNormalizeRangeTanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_old_model = torch.load('./pretrained_model/model_F_SVHN_NormRange.tar')['best_model']\n",
    "f_old_dict = f_old_model.state_dict()\n",
    "f_new_model = digits_model.F(3,False)\n",
    "f_new_dict = f_new_model.state_dict()\n",
    "f_new_dict = {k: v for k, v in f_old_dict.items() if k in f_new_dict}\n",
    "f_old_dict.update(f_new_dict)\n",
    "f_new_model.load_state_dict(f_new_dict)\n",
    "f_model = f_new_model\n",
    "\n",
    "for param in f_model.parameters():\n",
    "    param.requires_grad = False\n",
    "f_model = f_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model = torch.load('./final_models/fin_model.tar')['best_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVHN_transform = transforms.Compose([transforms.ToTensor(), NormalizeRangeTanh()])\n",
    "s_train_set = limits.LimitDataset(torchvision.datasets.SVHN(root = './data/svhn', split='extra',download = False, transform = SVHN_transform), 1024)\n",
    "s_train_loader = torch.utils.data.DataLoader(s_train_set, batch_size=128, shuffle=True, num_workers=8)\n",
    "\n",
    "s_test_set = limits.LimitDataset(torchvision.datasets.SVHN(root = './data/svhn/', split='test', download = False, transform = SVHN_transform),256)\n",
    "s_test_loader = torch.utils.data.DataLoader(s_test_set, batch_size=128, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "def make_image(M):\n",
    "    tList = [torchvision.utils.make_grid(unnormRange(m[:16]), nrow=4) for m in torch.unbind(M, dim=0) ]\n",
    "    res = torch.stack(tList, dim=0)\n",
    "    return res\n",
    "\n",
    "def train(classifier, device, train_loader, optimizer, epoch):\n",
    "    classifier.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Call Generator\n",
    "        #data = Variable(data.cpu().float())\n",
    "        s_f = f_model(data)\n",
    "        s_g = model['G'].cpu()(s_f)\n",
    "        #print(s_g.size())\n",
    "        #apply(torch.inverse, torch.randn(100, 200, 200))\n",
    "        s_g = s_g[:, :, 2:30, 2:30]\n",
    "        #s_g = make_image(s_g)\n",
    "        #s_g = torchvision.utils.make_grid(unnormRange(s_g[:16]), nrow=4)\n",
    "        #print(s_g.size())\n",
    "        if torch.cuda.is_available():\n",
    "            s_g.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = classifier(s_g)\n",
    "        #print(output.size(), target.size())\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(classifier, device, test_loader):\n",
    "    classifier.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Call Generator\n",
    "            data = Variable(data.cpu().float())\n",
    "            s_f = f_model(data)\n",
    "            s_g = model['G'].cpu()(s_f)\n",
    "            s_g = s_g[:, :, 2:30, 2:30]\n",
    "            if torch.cuda.is_available():\n",
    "                s_g.cuda()\n",
    "            output = classifier(s_g)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = False#torch.cuda.is_available()\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Net().to(device)\n",
    "optimizer = optim.SGD(classifier.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1024 (0%)]\tLoss: 2.308288\n",
      "\n",
      "Test set: Average loss: 2.2846, Accuracy: 28/256 (11%)\n",
      "\n",
      "Train Epoch: 2 [0/1024 (0%)]\tLoss: 2.306435\n",
      "\n",
      "Test set: Average loss: 2.2700, Accuracy: 43/256 (17%)\n",
      "\n",
      "Train Epoch: 3 [0/1024 (0%)]\tLoss: 2.271089\n",
      "\n",
      "Test set: Average loss: 2.2575, Accuracy: 75/256 (29%)\n",
      "\n",
      "Train Epoch: 4 [0/1024 (0%)]\tLoss: 2.272024\n",
      "\n",
      "Test set: Average loss: 2.2430, Accuracy: 98/256 (38%)\n",
      "\n",
      "Train Epoch: 5 [0/1024 (0%)]\tLoss: 2.292331\n",
      "\n",
      "Test set: Average loss: 2.2264, Accuracy: 79/256 (31%)\n",
      "\n",
      "Train Epoch: 6 [0/1024 (0%)]\tLoss: 2.246578\n",
      "\n",
      "Test set: Average loss: 2.2070, Accuracy: 77/256 (30%)\n",
      "\n",
      "Train Epoch: 7 [0/1024 (0%)]\tLoss: 2.217140\n",
      "\n",
      "Test set: Average loss: 2.1845, Accuracy: 61/256 (24%)\n",
      "\n",
      "Train Epoch: 8 [0/1024 (0%)]\tLoss: 2.205871\n",
      "\n",
      "Test set: Average loss: 2.1595, Accuracy: 59/256 (23%)\n",
      "\n",
      "Train Epoch: 9 [0/1024 (0%)]\tLoss: 2.179759\n",
      "\n",
      "Test set: Average loss: 2.1324, Accuracy: 66/256 (26%)\n",
      "\n",
      "Train Epoch: 10 [0/1024 (0%)]\tLoss: 2.186672\n",
      "\n",
      "Test set: Average loss: 2.1061, Accuracy: 77/256 (30%)\n",
      "\n",
      "Train Epoch: 11 [0/1024 (0%)]\tLoss: 2.093487\n",
      "\n",
      "Test set: Average loss: 2.0742, Accuracy: 83/256 (32%)\n",
      "\n",
      "Train Epoch: 12 [0/1024 (0%)]\tLoss: 2.132818\n",
      "\n",
      "Test set: Average loss: 2.0418, Accuracy: 94/256 (37%)\n",
      "\n",
      "Train Epoch: 13 [0/1024 (0%)]\tLoss: 2.071337\n",
      "\n",
      "Test set: Average loss: 1.9959, Accuracy: 97/256 (38%)\n",
      "\n",
      "Train Epoch: 14 [0/1024 (0%)]\tLoss: 2.068600\n",
      "\n",
      "Test set: Average loss: 1.9436, Accuracy: 109/256 (43%)\n",
      "\n",
      "Train Epoch: 15 [0/1024 (0%)]\tLoss: 1.967786\n",
      "\n",
      "Test set: Average loss: 1.8825, Accuracy: 129/256 (50%)\n",
      "\n",
      "Train Epoch: 16 [0/1024 (0%)]\tLoss: 1.924762\n",
      "\n",
      "Test set: Average loss: 1.8124, Accuracy: 137/256 (54%)\n",
      "\n",
      "Train Epoch: 17 [0/1024 (0%)]\tLoss: 1.924790\n",
      "\n",
      "Test set: Average loss: 1.7319, Accuracy: 144/256 (56%)\n",
      "\n",
      "Train Epoch: 18 [0/1024 (0%)]\tLoss: 1.848515\n",
      "\n",
      "Test set: Average loss: 1.6380, Accuracy: 152/256 (59%)\n",
      "\n",
      "Train Epoch: 19 [0/1024 (0%)]\tLoss: 1.662500\n",
      "\n",
      "Test set: Average loss: 1.5399, Accuracy: 163/256 (64%)\n",
      "\n",
      "Train Epoch: 20 [0/1024 (0%)]\tLoss: 1.655895\n",
      "\n",
      "Test set: Average loss: 1.4385, Accuracy: 167/256 (65%)\n",
      "\n",
      "Train Epoch: 21 [0/1024 (0%)]\tLoss: 1.567130\n",
      "\n",
      "Test set: Average loss: 1.3507, Accuracy: 169/256 (66%)\n",
      "\n",
      "Train Epoch: 22 [0/1024 (0%)]\tLoss: 1.343676\n",
      "\n",
      "Test set: Average loss: 1.2578, Accuracy: 176/256 (69%)\n",
      "\n",
      "Train Epoch: 23 [0/1024 (0%)]\tLoss: 1.333798\n",
      "\n",
      "Test set: Average loss: 1.1823, Accuracy: 176/256 (69%)\n",
      "\n",
      "Train Epoch: 24 [0/1024 (0%)]\tLoss: 1.297251\n",
      "\n",
      "Test set: Average loss: 1.1114, Accuracy: 186/256 (73%)\n",
      "\n",
      "Train Epoch: 25 [0/1024 (0%)]\tLoss: 1.104559\n",
      "\n",
      "Test set: Average loss: 1.0499, Accuracy: 191/256 (75%)\n",
      "\n",
      "Train Epoch: 26 [0/1024 (0%)]\tLoss: 1.100602\n",
      "\n",
      "Test set: Average loss: 0.9925, Accuracy: 198/256 (77%)\n",
      "\n",
      "Train Epoch: 27 [0/1024 (0%)]\tLoss: 1.164876\n",
      "\n",
      "Test set: Average loss: 0.9359, Accuracy: 207/256 (81%)\n",
      "\n",
      "Train Epoch: 28 [0/1024 (0%)]\tLoss: 1.041371\n",
      "\n",
      "Test set: Average loss: 0.8967, Accuracy: 198/256 (77%)\n",
      "\n",
      "Train Epoch: 29 [0/1024 (0%)]\tLoss: 0.879593\n",
      "\n",
      "Test set: Average loss: 0.8602, Accuracy: 206/256 (80%)\n",
      "\n",
      "Train Epoch: 30 [0/1024 (0%)]\tLoss: 0.808360\n",
      "\n",
      "Test set: Average loss: 0.8293, Accuracy: 202/256 (79%)\n",
      "\n",
      "Train Epoch: 31 [0/1024 (0%)]\tLoss: 0.771000\n",
      "\n",
      "Test set: Average loss: 0.8163, Accuracy: 201/256 (79%)\n",
      "\n",
      "Train Epoch: 32 [0/1024 (0%)]\tLoss: 0.815662\n",
      "\n",
      "Test set: Average loss: 0.7954, Accuracy: 204/256 (80%)\n",
      "\n",
      "Train Epoch: 33 [0/1024 (0%)]\tLoss: 0.678503\n",
      "\n",
      "Test set: Average loss: 0.7736, Accuracy: 204/256 (80%)\n",
      "\n",
      "Train Epoch: 34 [0/1024 (0%)]\tLoss: 0.854657\n",
      "\n",
      "Test set: Average loss: 0.7739, Accuracy: 204/256 (80%)\n",
      "\n",
      "Train Epoch: 35 [0/1024 (0%)]\tLoss: 0.666961\n",
      "\n",
      "Test set: Average loss: 0.7624, Accuracy: 206/256 (80%)\n",
      "\n",
      "Train Epoch: 36 [0/1024 (0%)]\tLoss: 0.652891\n",
      "\n",
      "Test set: Average loss: 0.7331, Accuracy: 211/256 (82%)\n",
      "\n",
      "Train Epoch: 37 [0/1024 (0%)]\tLoss: 0.776354\n",
      "\n",
      "Test set: Average loss: 0.7453, Accuracy: 206/256 (80%)\n",
      "\n",
      "Train Epoch: 38 [0/1024 (0%)]\tLoss: 0.487041\n",
      "\n",
      "Test set: Average loss: 0.7292, Accuracy: 205/256 (80%)\n",
      "\n",
      "Train Epoch: 39 [0/1024 (0%)]\tLoss: 0.671103\n",
      "\n",
      "Test set: Average loss: 0.7337, Accuracy: 204/256 (80%)\n",
      "\n",
      "Train Epoch: 40 [0/1024 (0%)]\tLoss: 0.626768\n",
      "\n",
      "Test set: Average loss: 0.7307, Accuracy: 207/256 (81%)\n",
      "\n",
      "Train Epoch: 41 [0/1024 (0%)]\tLoss: 0.438776\n",
      "\n",
      "Test set: Average loss: 0.7389, Accuracy: 204/256 (80%)\n",
      "\n",
      "Train Epoch: 42 [0/1024 (0%)]\tLoss: 0.500714\n",
      "\n",
      "Test set: Average loss: 0.7270, Accuracy: 202/256 (79%)\n",
      "\n",
      "Train Epoch: 43 [0/1024 (0%)]\tLoss: 0.444806\n",
      "\n",
      "Test set: Average loss: 0.7229, Accuracy: 207/256 (81%)\n",
      "\n",
      "Train Epoch: 44 [0/1024 (0%)]\tLoss: 0.421428\n",
      "\n",
      "Test set: Average loss: 0.7219, Accuracy: 206/256 (80%)\n",
      "\n",
      "Train Epoch: 45 [0/1024 (0%)]\tLoss: 0.459678\n",
      "\n",
      "Test set: Average loss: 0.7077, Accuracy: 207/256 (81%)\n",
      "\n",
      "Train Epoch: 46 [0/1024 (0%)]\tLoss: 0.495211\n",
      "\n",
      "Test set: Average loss: 0.7054, Accuracy: 208/256 (81%)\n",
      "\n",
      "Train Epoch: 47 [0/1024 (0%)]\tLoss: 0.442891\n",
      "\n",
      "Test set: Average loss: 0.7039, Accuracy: 210/256 (82%)\n",
      "\n",
      "Train Epoch: 48 [0/1024 (0%)]\tLoss: 0.330905\n",
      "\n",
      "Test set: Average loss: 0.7019, Accuracy: 203/256 (79%)\n",
      "\n",
      "Train Epoch: 49 [0/1024 (0%)]\tLoss: 0.455441\n",
      "\n",
      "Test set: Average loss: 0.7125, Accuracy: 208/256 (81%)\n",
      "\n",
      "Train Epoch: 50 [0/1024 (0%)]\tLoss: 0.392262\n",
      "\n",
      "Test set: Average loss: 0.7021, Accuracy: 207/256 (81%)\n",
      "\n",
      "Train Epoch: 51 [0/1024 (0%)]\tLoss: 0.456951\n",
      "\n",
      "Test set: Average loss: 0.7002, Accuracy: 213/256 (83%)\n",
      "\n",
      "Train Epoch: 52 [0/1024 (0%)]\tLoss: 0.475588\n",
      "\n",
      "Test set: Average loss: 0.7204, Accuracy: 207/256 (81%)\n",
      "\n",
      "Train Epoch: 53 [0/1024 (0%)]\tLoss: 0.505206\n",
      "\n",
      "Test set: Average loss: 0.7143, Accuracy: 208/256 (81%)\n",
      "\n",
      "Train Epoch: 54 [0/1024 (0%)]\tLoss: 0.474901\n",
      "\n",
      "Test set: Average loss: 0.7044, Accuracy: 209/256 (82%)\n",
      "\n",
      "Train Epoch: 55 [0/1024 (0%)]\tLoss: 0.373264\n",
      "\n",
      "Test set: Average loss: 0.7179, Accuracy: 207/256 (81%)\n",
      "\n",
      "Train Epoch: 56 [0/1024 (0%)]\tLoss: 0.299625\n",
      "\n",
      "Test set: Average loss: 0.7248, Accuracy: 211/256 (82%)\n",
      "\n",
      "Train Epoch: 57 [0/1024 (0%)]\tLoss: 0.354374\n",
      "\n",
      "Test set: Average loss: 0.7240, Accuracy: 211/256 (82%)\n",
      "\n",
      "Train Epoch: 58 [0/1024 (0%)]\tLoss: 0.371315\n",
      "\n",
      "Test set: Average loss: 0.7277, Accuracy: 213/256 (83%)\n",
      "\n",
      "Train Epoch: 59 [0/1024 (0%)]\tLoss: 0.258250\n",
      "\n",
      "Test set: Average loss: 0.7140, Accuracy: 210/256 (82%)\n",
      "\n",
      "Train Epoch: 60 [0/1024 (0%)]\tLoss: 0.473902\n",
      "\n",
      "Test set: Average loss: 0.7085, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 61 [0/1024 (0%)]\tLoss: 0.266081\n",
      "\n",
      "Test set: Average loss: 0.7104, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 62 [0/1024 (0%)]\tLoss: 0.348086\n",
      "\n",
      "Test set: Average loss: 0.7243, Accuracy: 210/256 (82%)\n",
      "\n",
      "Train Epoch: 63 [0/1024 (0%)]\tLoss: 0.338523\n",
      "\n",
      "Test set: Average loss: 0.7291, Accuracy: 211/256 (82%)\n",
      "\n",
      "Train Epoch: 64 [0/1024 (0%)]\tLoss: 0.431135\n",
      "\n",
      "Test set: Average loss: 0.7283, Accuracy: 209/256 (82%)\n",
      "\n",
      "Train Epoch: 65 [0/1024 (0%)]\tLoss: 0.348340\n",
      "\n",
      "Test set: Average loss: 0.7208, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 66 [0/1024 (0%)]\tLoss: 0.290845\n",
      "\n",
      "Test set: Average loss: 0.7229, Accuracy: 211/256 (82%)\n",
      "\n",
      "Train Epoch: 67 [0/1024 (0%)]\tLoss: 0.334213\n",
      "\n",
      "Test set: Average loss: 0.7353, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 68 [0/1024 (0%)]\tLoss: 0.184855\n",
      "\n",
      "Test set: Average loss: 0.7392, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 69 [0/1024 (0%)]\tLoss: 0.427130\n",
      "\n",
      "Test set: Average loss: 0.7246, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 70 [0/1024 (0%)]\tLoss: 0.249519\n",
      "\n",
      "Test set: Average loss: 0.7279, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 71 [0/1024 (0%)]\tLoss: 0.345780\n",
      "\n",
      "Test set: Average loss: 0.7341, Accuracy: 213/256 (83%)\n",
      "\n",
      "Train Epoch: 72 [0/1024 (0%)]\tLoss: 0.309953\n",
      "\n",
      "Test set: Average loss: 0.7273, Accuracy: 213/256 (83%)\n",
      "\n",
      "Train Epoch: 73 [0/1024 (0%)]\tLoss: 0.364212\n",
      "\n",
      "Test set: Average loss: 0.7426, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 74 [0/1024 (0%)]\tLoss: 0.371182\n",
      "\n",
      "Test set: Average loss: 0.7323, Accuracy: 213/256 (83%)\n",
      "\n",
      "Train Epoch: 75 [0/1024 (0%)]\tLoss: 0.322901\n",
      "\n",
      "Test set: Average loss: 0.7326, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 76 [0/1024 (0%)]\tLoss: 0.203211\n",
      "\n",
      "Test set: Average loss: 0.7315, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 77 [0/1024 (0%)]\tLoss: 0.428649\n",
      "\n",
      "Test set: Average loss: 0.7406, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 78 [0/1024 (0%)]\tLoss: 0.219023\n",
      "\n",
      "Test set: Average loss: 0.7355, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 79 [0/1024 (0%)]\tLoss: 0.330716\n",
      "\n",
      "Test set: Average loss: 0.7371, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 80 [0/1024 (0%)]\tLoss: 0.340165\n",
      "\n",
      "Test set: Average loss: 0.7417, Accuracy: 213/256 (83%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 81 [0/1024 (0%)]\tLoss: 0.258921\n",
      "\n",
      "Test set: Average loss: 0.7332, Accuracy: 213/256 (83%)\n",
      "\n",
      "Train Epoch: 82 [0/1024 (0%)]\tLoss: 0.302326\n",
      "\n",
      "Test set: Average loss: 0.7384, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 83 [0/1024 (0%)]\tLoss: 0.296638\n",
      "\n",
      "Test set: Average loss: 0.7310, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 84 [0/1024 (0%)]\tLoss: 0.316100\n",
      "\n",
      "Test set: Average loss: 0.7418, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 85 [0/1024 (0%)]\tLoss: 0.193479\n",
      "\n",
      "Test set: Average loss: 0.7376, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 86 [0/1024 (0%)]\tLoss: 0.280392\n",
      "\n",
      "Test set: Average loss: 0.7408, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 87 [0/1024 (0%)]\tLoss: 0.338924\n",
      "\n",
      "Test set: Average loss: 0.7576, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 88 [0/1024 (0%)]\tLoss: 0.284719\n",
      "\n",
      "Test set: Average loss: 0.7572, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 89 [0/1024 (0%)]\tLoss: 0.257621\n",
      "\n",
      "Test set: Average loss: 0.7472, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 90 [0/1024 (0%)]\tLoss: 0.256580\n",
      "\n",
      "Test set: Average loss: 0.7410, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 91 [0/1024 (0%)]\tLoss: 0.361237\n",
      "\n",
      "Test set: Average loss: 0.7556, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 92 [0/1024 (0%)]\tLoss: 0.302590\n",
      "\n",
      "Test set: Average loss: 0.7617, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 93 [0/1024 (0%)]\tLoss: 0.371859\n",
      "\n",
      "Test set: Average loss: 0.7472, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 94 [0/1024 (0%)]\tLoss: 0.241612\n",
      "\n",
      "Test set: Average loss: 0.7540, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 95 [0/1024 (0%)]\tLoss: 0.303572\n",
      "\n",
      "Test set: Average loss: 0.7737, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 96 [0/1024 (0%)]\tLoss: 0.185256\n",
      "\n",
      "Test set: Average loss: 0.7589, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 97 [0/1024 (0%)]\tLoss: 0.295402\n",
      "\n",
      "Test set: Average loss: 0.7603, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 98 [0/1024 (0%)]\tLoss: 0.325872\n",
      "\n",
      "Test set: Average loss: 0.7675, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 99 [0/1024 (0%)]\tLoss: 0.205107\n",
      "\n",
      "Test set: Average loss: 0.7816, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 100 [0/1024 (0%)]\tLoss: 0.229959\n",
      "\n",
      "Test set: Average loss: 0.7825, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 101 [0/1024 (0%)]\tLoss: 0.202528\n",
      "\n",
      "Test set: Average loss: 0.7778, Accuracy: 218/256 (85%)\n",
      "\n",
      "Train Epoch: 102 [0/1024 (0%)]\tLoss: 0.192367\n",
      "\n",
      "Test set: Average loss: 0.7889, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 103 [0/1024 (0%)]\tLoss: 0.192915\n",
      "\n",
      "Test set: Average loss: 0.7763, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 104 [0/1024 (0%)]\tLoss: 0.267216\n",
      "\n",
      "Test set: Average loss: 0.7771, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 105 [0/1024 (0%)]\tLoss: 0.299914\n",
      "\n",
      "Test set: Average loss: 0.7719, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 106 [0/1024 (0%)]\tLoss: 0.199728\n",
      "\n",
      "Test set: Average loss: 0.7689, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 107 [0/1024 (0%)]\tLoss: 0.241013\n",
      "\n",
      "Test set: Average loss: 0.7750, Accuracy: 213/256 (83%)\n",
      "\n",
      "Train Epoch: 108 [0/1024 (0%)]\tLoss: 0.146330\n",
      "\n",
      "Test set: Average loss: 0.7901, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 109 [0/1024 (0%)]\tLoss: 0.298867\n",
      "\n",
      "Test set: Average loss: 0.8018, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 110 [0/1024 (0%)]\tLoss: 0.300215\n",
      "\n",
      "Test set: Average loss: 0.7974, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 111 [0/1024 (0%)]\tLoss: 0.319944\n",
      "\n",
      "Test set: Average loss: 0.7978, Accuracy: 218/256 (85%)\n",
      "\n",
      "Train Epoch: 112 [0/1024 (0%)]\tLoss: 0.287050\n",
      "\n",
      "Test set: Average loss: 0.8112, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 113 [0/1024 (0%)]\tLoss: 0.179945\n",
      "\n",
      "Test set: Average loss: 0.7836, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 114 [0/1024 (0%)]\tLoss: 0.217565\n",
      "\n",
      "Test set: Average loss: 0.7983, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 115 [0/1024 (0%)]\tLoss: 0.176231\n",
      "\n",
      "Test set: Average loss: 0.8067, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 116 [0/1024 (0%)]\tLoss: 0.165460\n",
      "\n",
      "Test set: Average loss: 0.7992, Accuracy: 218/256 (85%)\n",
      "\n",
      "Train Epoch: 117 [0/1024 (0%)]\tLoss: 0.298180\n",
      "\n",
      "Test set: Average loss: 0.8052, Accuracy: 218/256 (85%)\n",
      "\n",
      "Train Epoch: 118 [0/1024 (0%)]\tLoss: 0.141368\n",
      "\n",
      "Test set: Average loss: 0.8061, Accuracy: 218/256 (85%)\n",
      "\n",
      "Train Epoch: 119 [0/1024 (0%)]\tLoss: 0.149522\n",
      "\n",
      "Test set: Average loss: 0.8014, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 120 [0/1024 (0%)]\tLoss: 0.264812\n",
      "\n",
      "Test set: Average loss: 0.7936, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 121 [0/1024 (0%)]\tLoss: 0.192798\n",
      "\n",
      "Test set: Average loss: 0.7886, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 122 [0/1024 (0%)]\tLoss: 0.207399\n",
      "\n",
      "Test set: Average loss: 0.8065, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 123 [0/1024 (0%)]\tLoss: 0.198005\n",
      "\n",
      "Test set: Average loss: 0.8010, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 124 [0/1024 (0%)]\tLoss: 0.245578\n",
      "\n",
      "Test set: Average loss: 0.7857, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 125 [0/1024 (0%)]\tLoss: 0.295861\n",
      "\n",
      "Test set: Average loss: 0.7985, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 126 [0/1024 (0%)]\tLoss: 0.176113\n",
      "\n",
      "Test set: Average loss: 0.8129, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 127 [0/1024 (0%)]\tLoss: 0.096021\n",
      "\n",
      "Test set: Average loss: 0.7933, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 128 [0/1024 (0%)]\tLoss: 0.255235\n",
      "\n",
      "Test set: Average loss: 0.7933, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 129 [0/1024 (0%)]\tLoss: 0.246862\n",
      "\n",
      "Test set: Average loss: 0.7994, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 130 [0/1024 (0%)]\tLoss: 0.247828\n",
      "\n",
      "Test set: Average loss: 0.8159, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 131 [0/1024 (0%)]\tLoss: 0.322709\n",
      "\n",
      "Test set: Average loss: 0.8088, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 132 [0/1024 (0%)]\tLoss: 0.291092\n",
      "\n",
      "Test set: Average loss: 0.8136, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 133 [0/1024 (0%)]\tLoss: 0.193312\n",
      "\n",
      "Test set: Average loss: 0.8197, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 134 [0/1024 (0%)]\tLoss: 0.281152\n",
      "\n",
      "Test set: Average loss: 0.8228, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 135 [0/1024 (0%)]\tLoss: 0.268008\n",
      "\n",
      "Test set: Average loss: 0.8238, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 136 [0/1024 (0%)]\tLoss: 0.103756\n",
      "\n",
      "Test set: Average loss: 0.8224, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 137 [0/1024 (0%)]\tLoss: 0.163526\n",
      "\n",
      "Test set: Average loss: 0.8253, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 138 [0/1024 (0%)]\tLoss: 0.178768\n",
      "\n",
      "Test set: Average loss: 0.8218, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 139 [0/1024 (0%)]\tLoss: 0.263204\n",
      "\n",
      "Test set: Average loss: 0.8361, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 140 [0/1024 (0%)]\tLoss: 0.193112\n",
      "\n",
      "Test set: Average loss: 0.8332, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 141 [0/1024 (0%)]\tLoss: 0.179416\n",
      "\n",
      "Test set: Average loss: 0.8346, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 142 [0/1024 (0%)]\tLoss: 0.183495\n",
      "\n",
      "Test set: Average loss: 0.8325, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 143 [0/1024 (0%)]\tLoss: 0.216760\n",
      "\n",
      "Test set: Average loss: 0.8172, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 144 [0/1024 (0%)]\tLoss: 0.271080\n",
      "\n",
      "Test set: Average loss: 0.8250, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 145 [0/1024 (0%)]\tLoss: 0.185961\n",
      "\n",
      "Test set: Average loss: 0.8304, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 146 [0/1024 (0%)]\tLoss: 0.233258\n",
      "\n",
      "Test set: Average loss: 0.8189, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 147 [0/1024 (0%)]\tLoss: 0.185098\n",
      "\n",
      "Test set: Average loss: 0.8462, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 148 [0/1024 (0%)]\tLoss: 0.166010\n",
      "\n",
      "Test set: Average loss: 0.8430, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 149 [0/1024 (0%)]\tLoss: 0.279475\n",
      "\n",
      "Test set: Average loss: 0.8469, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 150 [0/1024 (0%)]\tLoss: 0.204389\n",
      "\n",
      "Test set: Average loss: 0.8518, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 151 [0/1024 (0%)]\tLoss: 0.247848\n",
      "\n",
      "Test set: Average loss: 0.8552, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 152 [0/1024 (0%)]\tLoss: 0.162258\n",
      "\n",
      "Test set: Average loss: 0.8351, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 153 [0/1024 (0%)]\tLoss: 0.089281\n",
      "\n",
      "Test set: Average loss: 0.8480, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 154 [0/1024 (0%)]\tLoss: 0.186658\n",
      "\n",
      "Test set: Average loss: 0.8498, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 155 [0/1024 (0%)]\tLoss: 0.216818\n",
      "\n",
      "Test set: Average loss: 0.8356, Accuracy: 218/256 (85%)\n",
      "\n",
      "Train Epoch: 156 [0/1024 (0%)]\tLoss: 0.282966\n",
      "\n",
      "Test set: Average loss: 0.8427, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 157 [0/1024 (0%)]\tLoss: 0.259771\n",
      "\n",
      "Test set: Average loss: 0.8526, Accuracy: 213/256 (83%)\n",
      "\n",
      "Train Epoch: 158 [0/1024 (0%)]\tLoss: 0.201108\n",
      "\n",
      "Test set: Average loss: 0.8578, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 159 [0/1024 (0%)]\tLoss: 0.125622\n",
      "\n",
      "Test set: Average loss: 0.8518, Accuracy: 215/256 (84%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 160 [0/1024 (0%)]\tLoss: 0.284440\n",
      "\n",
      "Test set: Average loss: 0.8469, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 161 [0/1024 (0%)]\tLoss: 0.133811\n",
      "\n",
      "Test set: Average loss: 0.8595, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 162 [0/1024 (0%)]\tLoss: 0.122552\n",
      "\n",
      "Test set: Average loss: 0.8609, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 163 [0/1024 (0%)]\tLoss: 0.213248\n",
      "\n",
      "Test set: Average loss: 0.8727, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 164 [0/1024 (0%)]\tLoss: 0.116828\n",
      "\n",
      "Test set: Average loss: 0.8443, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 165 [0/1024 (0%)]\tLoss: 0.136633\n",
      "\n",
      "Test set: Average loss: 0.8433, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 166 [0/1024 (0%)]\tLoss: 0.185622\n",
      "\n",
      "Test set: Average loss: 0.8665, Accuracy: 219/256 (86%)\n",
      "\n",
      "Train Epoch: 167 [0/1024 (0%)]\tLoss: 0.117961\n",
      "\n",
      "Test set: Average loss: 0.8607, Accuracy: 218/256 (85%)\n",
      "\n",
      "Train Epoch: 168 [0/1024 (0%)]\tLoss: 0.167641\n",
      "\n",
      "Test set: Average loss: 0.8584, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 169 [0/1024 (0%)]\tLoss: 0.224593\n",
      "\n",
      "Test set: Average loss: 0.8594, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 170 [0/1024 (0%)]\tLoss: 0.236789\n",
      "\n",
      "Test set: Average loss: 0.8711, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 171 [0/1024 (0%)]\tLoss: 0.168502\n",
      "\n",
      "Test set: Average loss: 0.8770, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 172 [0/1024 (0%)]\tLoss: 0.178843\n",
      "\n",
      "Test set: Average loss: 0.8766, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 173 [0/1024 (0%)]\tLoss: 0.143283\n",
      "\n",
      "Test set: Average loss: 0.8997, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 174 [0/1024 (0%)]\tLoss: 0.229449\n",
      "\n",
      "Test set: Average loss: 0.8894, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 175 [0/1024 (0%)]\tLoss: 0.302209\n",
      "\n",
      "Test set: Average loss: 0.8923, Accuracy: 213/256 (83%)\n",
      "\n",
      "Train Epoch: 176 [0/1024 (0%)]\tLoss: 0.220734\n",
      "\n",
      "Test set: Average loss: 0.8844, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 177 [0/1024 (0%)]\tLoss: 0.237464\n",
      "\n",
      "Test set: Average loss: 0.8981, Accuracy: 213/256 (83%)\n",
      "\n",
      "Train Epoch: 178 [0/1024 (0%)]\tLoss: 0.130517\n",
      "\n",
      "Test set: Average loss: 0.9219, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 179 [0/1024 (0%)]\tLoss: 0.266368\n",
      "\n",
      "Test set: Average loss: 0.8972, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 180 [0/1024 (0%)]\tLoss: 0.179005\n",
      "\n",
      "Test set: Average loss: 0.8829, Accuracy: 213/256 (83%)\n",
      "\n",
      "Train Epoch: 181 [0/1024 (0%)]\tLoss: 0.205666\n",
      "\n",
      "Test set: Average loss: 0.9123, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 182 [0/1024 (0%)]\tLoss: 0.203654\n",
      "\n",
      "Test set: Average loss: 0.8839, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 183 [0/1024 (0%)]\tLoss: 0.194148\n",
      "\n",
      "Test set: Average loss: 0.8812, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 184 [0/1024 (0%)]\tLoss: 0.217020\n",
      "\n",
      "Test set: Average loss: 0.8642, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 185 [0/1024 (0%)]\tLoss: 0.198670\n",
      "\n",
      "Test set: Average loss: 0.8841, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 186 [0/1024 (0%)]\tLoss: 0.146105\n",
      "\n",
      "Test set: Average loss: 0.9035, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 187 [0/1024 (0%)]\tLoss: 0.147415\n",
      "\n",
      "Test set: Average loss: 0.9149, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 188 [0/1024 (0%)]\tLoss: 0.283551\n",
      "\n",
      "Test set: Average loss: 0.9180, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 189 [0/1024 (0%)]\tLoss: 0.201717\n",
      "\n",
      "Test set: Average loss: 0.8956, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 190 [0/1024 (0%)]\tLoss: 0.248566\n",
      "\n",
      "Test set: Average loss: 0.8959, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 191 [0/1024 (0%)]\tLoss: 0.149383\n",
      "\n",
      "Test set: Average loss: 0.8980, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 192 [0/1024 (0%)]\tLoss: 0.122414\n",
      "\n",
      "Test set: Average loss: 0.8986, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 193 [0/1024 (0%)]\tLoss: 0.177521\n",
      "\n",
      "Test set: Average loss: 0.8933, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 194 [0/1024 (0%)]\tLoss: 0.110818\n",
      "\n",
      "Test set: Average loss: 0.8922, Accuracy: 215/256 (84%)\n",
      "\n",
      "Train Epoch: 195 [0/1024 (0%)]\tLoss: 0.165308\n",
      "\n",
      "Test set: Average loss: 0.8996, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 196 [0/1024 (0%)]\tLoss: 0.254745\n",
      "\n",
      "Test set: Average loss: 0.8997, Accuracy: 217/256 (85%)\n",
      "\n",
      "Train Epoch: 197 [0/1024 (0%)]\tLoss: 0.196418\n",
      "\n",
      "Test set: Average loss: 0.9097, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 198 [0/1024 (0%)]\tLoss: 0.148315\n",
      "\n",
      "Test set: Average loss: 0.9139, Accuracy: 214/256 (84%)\n",
      "\n",
      "Train Epoch: 199 [0/1024 (0%)]\tLoss: 0.179961\n",
      "\n",
      "Test set: Average loss: 0.9146, Accuracy: 216/256 (84%)\n",
      "\n",
      "Train Epoch: 200 [0/1024 (0%)]\tLoss: 0.152881\n",
      "\n",
      "Test set: Average loss: 0.9341, Accuracy: 215/256 (84%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(classifier, device, s_train_loader, optimizer, epoch)\n",
    "    test(classifier, device, s_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
